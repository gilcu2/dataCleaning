20/04/20 17:55:46 WARN util.Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.1.129 instead (on interface wlp0s20f3)
20/04/20 17:55:46 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
20/04/20 17:55:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/04/20 17:55:46 INFO gilcu2.DataCleaningMain$: Begin: 2020-04-20T17:55:46.595+02:00
20/04/20 17:55:46 INFO gilcu2.DataCleaningMain$: Arguments: [hdfs:/user/gilcu2/bdp/sample_1.csv]
20/04/20 17:55:46 INFO spark.SparkContext: Running Spark version 2.4.4
20/04/20 17:55:46 INFO spark.SparkContext: Submitted application: DataCleaning
20/04/20 17:55:46 INFO spark.SecurityManager: Changing view acls to: gilcu2
20/04/20 17:55:46 INFO spark.SecurityManager: Changing modify acls to: gilcu2
20/04/20 17:55:46 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/20 17:55:46 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/20 17:55:46 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(gilcu2); groups with view permissions: Set(); users  with modify permissions: Set(gilcu2); groups with modify permissions: Set()
20/04/20 17:55:46 INFO util.Utils: Successfully started service 'sparkDriver' on port 41153.
20/04/20 17:55:46 INFO spark.SparkEnv: Registering MapOutputTracker
20/04/20 17:55:46 INFO spark.SparkEnv: Registering BlockManagerMaster
20/04/20 17:55:46 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/04/20 17:55:46 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/04/20 17:55:46 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-b14e6d46-0714-4b84-9b84-4df235940473
20/04/20 17:55:46 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
20/04/20 17:55:46 INFO spark.SparkEnv: Registering OutputCommitCoordinator
20/04/20 17:55:46 INFO util.log: Logging initialized @1178ms
20/04/20 17:55:46 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
20/04/20 17:55:46 INFO server.Server: Started @1217ms
20/04/20 17:55:46 INFO server.AbstractConnector: Started ServerConnector@6a9287b1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20/04/20 17:55:46 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1286528d{/jobs,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@705202d1{/jobs/json,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c443976{/jobs/job,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3bd55d8{/jobs/job/json,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63998bf4{/stages,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e0b9178{/stages/json,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61942c1{/stages/stage,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62a8fd44{/stages/stage/json,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2e6ba49a{/stages/pool,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f5b5ca4{/stages/pool/json,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4ee33af7{/storage,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b04acb2{/storage/json,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18388a3c{/storage/rdd,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d035be3{/storage/rdd/json,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a60ee36{/environment,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4cfbaf4{/environment/json,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58faa93b{/executors,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f212d84{/executors/json,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@127d7908{/executors/threadDump,null,AVAILABLE,@Spark}
20/04/20 17:55:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b9c69a9{/executors/threadDump/json,null,AVAILABLE,@Spark}
20/04/20 17:55:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6622a690{/static,null,AVAILABLE,@Spark}
20/04/20 17:55:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27cbfddf{/,null,AVAILABLE,@Spark}
20/04/20 17:55:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27ead29e{/api,null,AVAILABLE,@Spark}
20/04/20 17:55:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49b07ee3{/jobs/job/kill,null,AVAILABLE,@Spark}
20/04/20 17:55:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@352e612e{/stages/stage/kill,null,AVAILABLE,@Spark}
20/04/20 17:55:47 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.129:4040
20/04/20 17:55:47 INFO spark.SparkContext: Added JAR file:/home/gilcu2/prog/dataCleaning/target/scala-2.11/DataCleaning.jar at spark://192.168.1.129:41153/jars/DataCleaning.jar with timestamp 1587398147016
20/04/20 17:55:47 INFO executor.Executor: Starting executor ID driver on host localhost
20/04/20 17:55:47 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42679.
20/04/20 17:55:47 INFO netty.NettyBlockTransferService: Server created on 192.168.1.129:42679
20/04/20 17:55:47 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/20 17:55:47 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.129, 42679, None)
20/04/20 17:55:47 INFO storage.BlockManagerMasterEndpoint: Registering block manager 192.168.1.129:42679 with 366.3 MB RAM, BlockManagerId(driver, 192.168.1.129, 42679, None)
20/04/20 17:55:47 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.129, 42679, None)
20/04/20 17:55:47 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.129, 42679, None)
20/04/20 17:55:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@236134a1{/metrics/json,null,AVAILABLE,@Spark}
20/04/20 17:55:47 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/gilcu2/prog/dataCleaning/spark-warehouse').
20/04/20 17:55:47 INFO internal.SharedState: Warehouse path is 'file:/home/gilcu2/prog/dataCleaning/spark-warehouse'.
20/04/20 17:55:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4cb0a000{/SQL,null,AVAILABLE,@Spark}
20/04/20 17:55:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ff2e84b{/SQL/json,null,AVAILABLE,@Spark}
20/04/20 17:55:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65a5d4f9{/SQL/execution,null,AVAILABLE,@Spark}
20/04/20 17:55:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40298285{/SQL/execution/json,null,AVAILABLE,@Spark}
20/04/20 17:55:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1fcf9739{/static/sql,null,AVAILABLE,@Spark}
20/04/20 17:55:47 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/04/20 17:55:49 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/20 17:55:49 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
20/04/20 17:55:49 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/20 17:55:49 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/20 17:55:49 INFO codegen.CodeGenerator: Code generated in 121.084116 ms
20/04/20 17:55:49 INFO codegen.CodeGenerator: Code generated in 11.40592 ms
20/04/20 17:55:49 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 286.8 KB, free 366.0 MB)
20/04/20 17:55:49 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.6 KB, free 366.0 MB)
20/04/20 17:55:49 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.129:42679 (size: 23.6 KB, free: 366.3 MB)
20/04/20 17:55:49 INFO spark.SparkContext: Created broadcast 0 from csv at DataCleaningMain.scala:39
20/04/20 17:55:49 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/04/20 17:55:49 INFO spark.SparkContext: Starting job: csv at DataCleaningMain.scala:39
20/04/20 17:55:49 INFO scheduler.DAGScheduler: Got job 0 (csv at DataCleaningMain.scala:39) with 1 output partitions
20/04/20 17:55:49 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DataCleaningMain.scala:39)
20/04/20 17:55:49 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/20 17:55:49 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/20 17:55:49 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at DataCleaningMain.scala:39), which has no missing parents
20/04/20 17:55:49 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.8 KB, free 366.0 MB)
20/04/20 17:55:49 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KB, free 366.0 MB)
20/04/20 17:55:49 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.129:42679 (size: 4.5 KB, free: 366.3 MB)
20/04/20 17:55:49 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
20/04/20 17:55:49 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at DataCleaningMain.scala:39) (first 15 tasks are for partitions Vector(0))
20/04/20 17:55:49 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
20/04/20 17:55:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 8266 bytes)
20/04/20 17:55:49 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
20/04/20 17:55:49 INFO executor.Executor: Fetching spark://192.168.1.129:41153/jars/DataCleaning.jar with timestamp 1587398147016
20/04/20 17:55:49 INFO client.TransportClientFactory: Successfully created connection to /192.168.1.129:41153 after 16 ms (0 ms spent in bootstraps)
20/04/20 17:55:49 INFO util.Utils: Fetching spark://192.168.1.129:41153/jars/DataCleaning.jar to /tmp/spark-978ec461-2bfe-4c4e-b55c-a6a15bc873d9/userFiles-66dc26a4-fcf6-40af-bf81-6cb8210c5be4/fetchFileTemp7629532017241476579.tmp
20/04/20 17:55:49 INFO executor.Executor: Adding file:/tmp/spark-978ec461-2bfe-4c4e-b55c-a6a15bc873d9/userFiles-66dc26a4-fcf6-40af-bf81-6cb8210c5be4/DataCleaning.jar to class loader
20/04/20 17:55:49 INFO datasources.FileScanRDD: Reading File path: hdfs://localhost:9000/user/gilcu2/bdp/sample_1.csv, range: 0-74, partition values: [empty row]
20/04/20 17:55:49 INFO codegen.CodeGenerator: Code generated in 5.598442 ms
20/04/20 17:55:50 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1275 bytes result sent to driver
20/04/20 17:55:50 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 185 ms on localhost (executor driver) (1/1)
20/04/20 17:55:50 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/04/20 17:55:50 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DataCleaningMain.scala:39) finished in 0.239 s
20/04/20 17:55:50 INFO scheduler.DAGScheduler: Job 0 finished: csv at DataCleaningMain.scala:39, took 0.264432 s
20/04/20 17:55:50 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/20 17:55:50 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
20/04/20 17:55:50 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/20 17:55:50 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/20 17:55:50 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 286.8 KB, free 365.7 MB)
20/04/20 17:55:50 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.6 KB, free 365.7 MB)
20/04/20 17:55:50 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.129:42679 (size: 23.6 KB, free: 366.2 MB)
20/04/20 17:55:50 INFO spark.SparkContext: Created broadcast 2 from csv at DataCleaningMain.scala:39
20/04/20 17:55:50 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/04/20 17:55:50 INFO codegen.CodeGenerator: Code generated in 6.378953 ms
20/04/20 17:55:50 INFO spark.SparkContext: Starting job: show at DataCleaningMain.scala:21
20/04/20 17:55:50 INFO scheduler.DAGScheduler: Got job 1 (show at DataCleaningMain.scala:21) with 1 output partitions
20/04/20 17:55:50 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (show at DataCleaningMain.scala:21)
20/04/20 17:55:50 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/20 17:55:50 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/20 17:55:50 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at show at DataCleaningMain.scala:21), which has no missing parents
20/04/20 17:55:50 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.6 KB, free 365.7 MB)
20/04/20 17:55:50 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.5 KB, free 365.7 MB)
20/04/20 17:55:50 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.129:42679 (size: 9.5 KB, free: 366.2 MB)
20/04/20 17:55:50 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
20/04/20 17:55:50 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at show at DataCleaningMain.scala:21) (first 15 tasks are for partitions Vector(0))
20/04/20 17:55:50 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/04/20 17:55:50 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 8266 bytes)
20/04/20 17:55:50 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
20/04/20 17:55:50 INFO codegen.CodeGenerator: Code generated in 5.735592 ms
20/04/20 17:55:50 INFO datasources.FileScanRDD: Reading File path: hdfs://localhost:9000/user/gilcu2/bdp/sample_1.csv, range: 0-74, partition values: [empty row]
20/04/20 17:55:50 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 1729 bytes result sent to driver
20/04/20 17:55:50 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 42 ms on localhost (executor driver) (1/1)
20/04/20 17:55:50 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/04/20 17:55:50 INFO scheduler.DAGScheduler: ResultStage 1 (show at DataCleaningMain.scala:21) finished in 0.062 s
20/04/20 17:55:50 INFO scheduler.DAGScheduler: Job 1 finished: show at DataCleaningMain.scala:21, took 0.064482 s
20/04/20 17:55:50 INFO gilcu2.DataCleaningMain$: End: 2020-04-20T17:55:50.208+02:00 Total: 3 seconds and 613 milliseconds
20/04/20 17:55:50 INFO spark.SparkContext: Invoking stop() from shutdown hook
20/04/20 17:55:50 INFO server.AbstractConnector: Stopped Spark@6a9287b1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20/04/20 17:55:50 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.1.129:4040
20/04/20 17:55:50 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/04/20 17:55:50 INFO memory.MemoryStore: MemoryStore cleared
20/04/20 17:55:50 INFO storage.BlockManager: BlockManager stopped
20/04/20 17:55:50 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
20/04/20 17:55:50 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/04/20 17:55:50 INFO spark.SparkContext: Successfully stopped SparkContext
20/04/20 17:55:50 INFO util.ShutdownHookManager: Shutdown hook called
20/04/20 17:55:50 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-68412191-d531-4ca2-944b-4587d2279ce5
20/04/20 17:55:50 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-978ec461-2bfe-4c4e-b55c-a6a15bc873d9
